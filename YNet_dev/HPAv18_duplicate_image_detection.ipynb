{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate image detection\n",
    "\n",
    "https://www.kaggle.com/iezepov/get-hash-from-images-slightly-daster\n",
    "\n",
    "https://blog.iconfinder.com/detecting-duplicate-images-using-python-cb240b05a3b6\n",
    "\n",
    "https://www.pyimagesearch.com/2017/11/27/image-hashing-opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from resources.conv_learner import *\n",
    "\n",
    "# required to get individual channels out of image object\n",
    "from PIL import Image, ImageSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv from images in folder\n",
    "\n",
    "def HPAv18_csv_from_dir(path, folder, outfile):\n",
    "    _fnames = read_dir(path, folder)\n",
    "    \n",
    "    # note use of rsplit (!)\n",
    "    df_fnames = pd.DataFrame([os.path.basename(_fnames[i]).split('.')[0].rsplit('_',1)[0] for i, _ in enumerate(_fnames)])\n",
    "    uni_fnames = df_fnames[0].drop_duplicates(); # only unique fnames\n",
    "   \n",
    "    \n",
    "    return uni_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/Kaggle_HPA_2018/'\n",
    "folder = 'HPA_multiproc_test'\n",
    "outfile = 'HPAv18_labels_test.csv'\n",
    "\n",
    "HPAv18_df = HPAv18_csv_from_dir(path, folder, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPAv18_df.to_csv(path + outfile, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate dhashes of images for 'needles' and 'haystack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from HPAv18_ImCompare import dhash\n",
    "\n",
    "def get_im_paths(path, folders):\n",
    "    \n",
    "    fnames = []\n",
    "    \n",
    "    for f in folders:\n",
    "        _fnames = read_dir(path, f)\n",
    "        _fnames = [[f, _fnames[i]] for i, _ in enumerate(_fnames)] # remove idx to get all !!!\n",
    "        fnames = fnames + _fnames\n",
    "    \n",
    "    res = np.vstack(fnames)\n",
    "#     df = pd.DataFrame(fnames, columns=['folder', 'Path_Id'])\n",
    "    print(f\"Total images: {len(res)}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_im_hash_ch0(im_paths):\n",
    "    \n",
    "    res = []\n",
    "    print(f\"dhashing images...\")\n",
    "\n",
    "    for im_name in tqdm(im_paths, total = len(im_paths), unit=\"files\"):\n",
    "        im = Image.open(path + im_name)\n",
    "\n",
    "        # might want to add options for different channels here\n",
    "        im_ch_0 = ImageSequence.Iterator(im)[0]\n",
    "\n",
    "        h = dhash(im_ch_0)\n",
    "        res.append([im_name, h])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 35\n",
      "Total images: 104\n",
      "dhashing images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd644fcf28e1408688658a782a4f736d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dhashing images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30b5c0640534553a365d794b26bc9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=104), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define sources\n",
    "path = 'datasets/Kaggle_HPA_2018/'\n",
    "needles_folders = ['test_needles']\n",
    "haystack_folders = ['test_haystack']\n",
    "\n",
    "# get image paths:\n",
    "needles_path = get_im_paths(path, needles_folders)\n",
    "haystack_path = get_im_paths(path, haystack_folders)\n",
    "\n",
    "# hash_list = get_im_hash_ch0(im_paths.Path_Id)\n",
    "needles = np.vstack(get_im_hash_ch0(needles_path[:,1]))\n",
    "haystack = np.vstack(get_im_hash_ch0(haystack_path[:,1]))\n",
    "\n",
    "# pd.DataFrame(hash_list, columns=['Id', 'Ch0_dhash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect duplicates based on hamming_distance between dhashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(s1, s2):\n",
    "    \"\"\"Return the Hamming distance between equal-length sequences\"\"\"\n",
    "    if len(s1) != len(s2):\n",
    "        raise ValueError(\"Undefined for sequences of unequal length\")\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "\n",
    "def find_dupes(needles, haystack):\n",
    "    dupes = []\n",
    "    counter = 0\n",
    "\n",
    "    for f, n in needles:\n",
    "        dist = np.array([hamming_distance(n,h) for h in haystack[:,1]])\n",
    "        idx = np.flatnonzero(dist < 10)\n",
    "        _dupes = haystack[:,0][idx]\n",
    "        if len(_dupes) > 0: # duplicate found!\n",
    "            counter += 1\n",
    "            dupes.append([f, _dupes, dist[idx]])\n",
    "\n",
    "    print(f'Duplicates found: {counter}')\n",
    "\n",
    "    return pd.DataFrame(dupes, columns=['Id','duplicates', 'hamming_dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>hamming_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_needles\\00ad3e84-bad1-11e8-b2b8-ac1f6b643...</td>\n",
       "      <td>[test_haystack\\00ad3e84-bad1-11e8-b2b8-ac1f6b6...</td>\n",
       "      <td>[0, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_needles\\1183_50_A11_1.tiff</td>\n",
       "      <td>[test_haystack\\1183_50_A11_1.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_needles\\1183_50_A11_2.tiff</td>\n",
       "      <td>[test_haystack\\1183_50_A11_2.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_needles\\1183_51_A11_1.tiff</td>\n",
       "      <td>[test_haystack\\1183_51_A11_1.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_needles\\1183_51_A11_2.tiff</td>\n",
       "      <td>[test_haystack\\1183_51_A11_2.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_needles\\1183_52_A11_1.tiff</td>\n",
       "      <td>[test_haystack\\1183_52_A11_1.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_needles\\1183_52_A11_2.tiff</td>\n",
       "      <td>[test_haystack\\1183_52_A11_2.tiff]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id  \\\n",
       "0  test_needles\\00ad3e84-bad1-11e8-b2b8-ac1f6b643...   \n",
       "1                    test_needles\\1183_50_A11_1.tiff   \n",
       "2                    test_needles\\1183_50_A11_2.tiff   \n",
       "3                    test_needles\\1183_51_A11_1.tiff   \n",
       "4                    test_needles\\1183_51_A11_2.tiff   \n",
       "5                    test_needles\\1183_52_A11_1.tiff   \n",
       "6                    test_needles\\1183_52_A11_2.tiff   \n",
       "\n",
       "                                          duplicates  hamming_dist  \n",
       "0  [test_haystack\\00ad3e84-bad1-11e8-b2b8-ac1f6b6...  [0, 0, 0, 2]  \n",
       "1                 [test_haystack\\1183_50_A11_1.tiff]           [0]  \n",
       "2                 [test_haystack\\1183_50_A11_2.tiff]           [0]  \n",
       "3                 [test_haystack\\1183_51_A11_1.tiff]           [0]  \n",
       "4                 [test_haystack\\1183_51_A11_2.tiff]           [0]  \n",
       "5                 [test_haystack\\1183_52_A11_1.tiff]           [0]  \n",
       "6                 [test_haystack\\1183_52_A11_2.tiff]           [0]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupe_df = find_dupes(needles, haystack)\n",
    "dupe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append purged_df to HPA_labels.csv and save as HPA_extended_data.csv\n",
    "# Option 1: delete duplicate images in HPAv18 folder; then proceed...\n",
    "# Option 2: bash copy all original data into HPAv18 (faster than other way around)\n",
    "# Option 3: leave files in respective folders; modify csv's to include partial path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dupes(csv_file):\n",
    "    \n",
    "    with open(path + csv_file, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        fnames = np.vstack(list(reader))  \n",
    "    \n",
    "    fnames_df_purge = pd.DataFrame(fnames[1:], columns=['Id', 'Target'])\n",
    "    orig_df = fnames_df_purge.copy()\n",
    "    \n",
    "    for im in dupe_df.duplicates:\n",
    "        for d in im:\n",
    "            dup = os.path.basename(d).split('.')[0]\n",
    "            dup_idx = fnames_df_purge[(fnames_df_purge.Id == dup)].index\n",
    "            fnames_df_purge = fnames_df_purge.drop(dup_idx)\n",
    "            \n",
    "    print(f'Total images removed: {len(orig_df) - len(purged_df)}')\n",
    "\n",
    "    return fnames_df_purge, orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images less: 6\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'HPAv18RBGY_wodpl.csv'\n",
    "purged_haystack_df, orig_df = remove_dupes(csv_file)\n",
    "\n",
    "# save purged csv:\n",
    "# purged_haystack_df.to_csv(path + 'HPAv18_labels_unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options 3: modifying csv's to include partial path to folders - this way, merging folders can be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HPA_labels.csv to add partial path to Id's\n",
    "def load_csv(path_to_csv):\n",
    "\n",
    "    with open(path + csv_file, 'r') as d:\n",
    "            reader = csv.reader(d)\n",
    "            HPA_labels = np.vstack(list(reader)) \n",
    "\n",
    "    return pd.DataFrame(HPA_labels, columns=['Id', 'Target'])\n",
    "\n",
    "\n",
    "# add partial path to Id's in csv's\n",
    "def add_partial_path(df):\n",
    "    for n in tqdm(df.Id, total = len(df.Id), unit=\"files\"):\n",
    "        df.Id.replace(n, 'test/' + n, inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPA_labels_file = path + 'HPA_labels.csv'\n",
    "HPAv18_uniques_labels_file = path + 'HPAv18_labels_unique.csv'\n",
    "\n",
    "HPA_labels_df = load_csv(HPA_labels_file)\n",
    "HPAv18_labels_df = load_csv(HPAv18_uniques_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_HPA = add_partial_path(HPA_labels_df)\n",
    "mod_HPAv18_purged = add_partial_path(HPAv18_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_HPA.to_csv(path + 'HPA_Kaggle_labels_with_path')\n",
    "# mod_HPAv18_purged.to_csv(path + 'HPAv18_labels_with_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in dupe_df.duplicates:\n",
    "    for d in im:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS DELETES FILES!!!\n",
    "\n",
    "for im in dupe_df.duplicates:\n",
    "    for d in im:\n",
    "        targ = (path + d)\n",
    "        \n",
    "#         !rm $targ # ---> activate to arm the function!!!!\n",
    "\n",
    "#         print(f'deleted duplicate: {os.path.basename(targ)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = needles[0][1]\n",
    "u = haystack[0][1]\n",
    "\n",
    "\n",
    "dist = np.array([hamming_distance(v,h) for h in haystack[:,1]])\n",
    "idx = np.flatnonzero(dist < 10) # better than np.where() ?!\n",
    "\n",
    "dada = '-'.join(list(haystack[:,0][idx]))\n",
    "dada\n",
    "# list(haystack[idx,0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling file-list from Kaggle_HPA_train, test and HPAv18_external\n",
    "\n",
    "def read_files(base_path, csvs):\n",
    "    \n",
    "    for f in folders:\n",
    "        \n",
    "        with open(path + f, 'r') as d:\n",
    "            read = list(csv.reader(d))\n",
    "            flist.append(read)    \n",
    "            \n",
    "    return np.vstack(flist[0])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
