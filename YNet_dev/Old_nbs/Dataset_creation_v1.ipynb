{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset from .zip files and calc normalization statistics\n",
    "\n",
    "Step_1: create standard dataset to be re-used, either by creating folders or as hdf5 object. \n",
    "\n",
    "Step_2: calculate mean + std for train and test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating shuffled list of train, test and potentially validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleZip(Dataset_name, input_path, output_path, val = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to unzip, shuffle, re-zip and store a set of images at a specified location.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    Dataset_name: name of the dataset, e.g. WT_175 - should be descriptive \n",
    "    input_path: path to input .zip file\n",
    "    output_path: path for shuffled .zip-file to be stored\n",
    "\n",
    "    -> creates temp folder in same directory as .zip file to store unzipped files in, but deletes it once done. \n",
    "    -> shuffles and splits unzipped files between train, test and optionally val datasets.\n",
    "    -> optionally re-zip or storage in hdf5 object (TODO)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "\n",
    "\n",
    "    ### OPTIONS ###\n",
    "    # select cell designation, e.g. WT or mfb1KO - important for, well, naming... \n",
    "    Dataset_name = 'mmm1KO_230' #don't add .zip here\n",
    "\n",
    "    # choose target .zip file\n",
    "    ZPath = 'datasets/Exp1_data_storage/original_zips/mmm1KO_230.zip'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Execution --------------------------------------------------------------------------------###\n",
    "    TempPath = os.path.dirname(ZPath) + '/TEMP-' + Dataset_name # Path definition, also for later use\n",
    "\n",
    "    # unzips files into temp folder\n",
    "    if os.path.exists(TempPath):\n",
    "        raise ValueError('temp folder already exists in directory; consider deleting and re-run')\n",
    "    else:\n",
    "        os.makedirs(TempPath)\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(ZPath, 'r') \n",
    "    zip_ref.extractall(TempPath)\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "random.seed(1) #reproducible randomness\n",
    "\n",
    "### OPTIONS ###\n",
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "val = False # TODO: optional creation of validation dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Execution --------------------------------------------------------------------------------###\n",
    "\n",
    "# get list of files in TempPath\n",
    "addrs = os.listdir(TempPath)\n",
    "\n",
    "# create shuffled list\n",
    "if shuffle_data:\n",
    "    addrs = random.sample(addrs, k = len(addrs)) #creates shuffled list by random sampling from original list.\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Question: \n",
    "Generating train, test and optionally val datasets - Question: should there be the same absolute number of test/val \n",
    "images for each class or should the number vary depending on total number of images per class e.g. \n",
    "20 test images for a total of 100 class A images, but 40 for a total of 200 class B images?   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# # Divide the hata into 60% train, 20% test, and optionally 20% val\n",
    "# train_addrs = addrs[0:int(0.8*len(addrs))]\n",
    "# test_addrs = addrs[int(0.8*len(addrs)):]\n",
    "# # val_addrs = addrs[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "\n",
    "# Select == 35 images for test and optionally val datasets; put the rest into train\n",
    "test_addrs = addrs[0:35]\n",
    "train_addrs = addrs[35:]\n",
    "\n",
    "print(len(train_addrs))\n",
    "print(len(test_addrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating .zip files from shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All training images zipped successfully!\n",
      "All test images zipped successfully!\n",
      "Files moved to:datasets/Exp1_data_storage/shuffled_zips/\n"
     ]
    }
   ],
   "source": [
    "# Creating .zip file of train, test and potentially validation images. \n",
    "\n",
    "from zipfile import ZipFile\n",
    "from os.path import basename #required to use in zipfile.Zipfile.write(file, basename(file)) to avoid completed path to be archived\n",
    "import shutil \n",
    "\n",
    "verbose = 0\n",
    "\n",
    "save_path = 'datasets/Exp1_data_storage/shuffled_zips/'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "### processing train images: ###\n",
    "if verbose:\n",
    "    print('Following files will be zipped:')\n",
    "    for addrs in train_addrs:\n",
    "        print(addrs)\n",
    "        \n",
    "# writing files to a zipfile\n",
    "with ZipFile(save_path + Dataset_name + '_train_data.zip','w') as zip:\n",
    "    # writing each file one by one\n",
    "    for addrs in train_addrs:\n",
    "        zip.write(TempPath + '/' + addrs, basename(addrs))\n",
    "        \n",
    "    print('All training images zipped successfully!')\n",
    "    \n",
    "    \n",
    "### processing Test images: ###\n",
    "if verbose:\n",
    "    print('Following files will be zipped:')\n",
    "    for addrs in test_addrs:\n",
    "        print(addrs)\n",
    "        \n",
    "# writing files to a zipfile\n",
    "with ZipFile(save_path + Dataset_name + '_test_data.zip','w') as zip:\n",
    "    # writing each file one by one\n",
    "    for addrs in test_addrs:\n",
    "        zip.write(TempPath + '/' + addrs, basename(addrs))\n",
    " \n",
    "    print('All test images zipped successfully!')\n",
    "    \n",
    "zip_ref.close()\n",
    "\n",
    "print('Files moved to:' + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train created\n",
      "train/mmm1KO_230 created\n",
      "test created\n",
      "test/mmm1KO_230 created\n"
     ]
    }
   ],
   "source": [
    "# To be used with shuffled data in zip files. \n",
    "# Extracts these to specified dataset folder in train/test subfolders\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "### OPTIONS ###\n",
    "# define path for files to be unzipped and stored in train and test directories\n",
    "dataset_path = 'datasets/yeast_v3/'\n",
    "\n",
    "# select cell designation, e.g. WT or mfb1KO - important for, well, naming... \n",
    "Dataset_name = 'mmm1KO_230' #don't add .zip here\n",
    "\n",
    "# choose path where target zip-files are stored\n",
    "ZPath = 'datasets/Exp1_data_storage/shuffled_zips'\n",
    "\n",
    "# optionally add 'val' keyword if datasets (zip files) have been created accordingly\n",
    "data_struct = ['train', 'test']\n",
    "\n",
    "\n",
    "\n",
    "### Execution --------------------------------------------------------------------------------###\n",
    "\n",
    "# unzips files correct folders or creates them\n",
    "\n",
    "for i in data_struct:\n",
    "\n",
    "    if not os.path.exists(dataset_path + '/' + i):\n",
    "        os.makedirs(dataset_path + '/' + i)\n",
    "        print(i + ' created')\n",
    "        if not os.path.exists(dataset_path + '/' + i + Dataset_name):\n",
    "            os.makedirs(dataset_path + '/' + i + '/' + Dataset_name)\n",
    "            print(i + '/' + Dataset_name + ' created')\n",
    "        else:\n",
    "            print('WARNING: *added* images to existing folder:' + i + '/' + Dataset_name)\n",
    "    \n",
    "    zip_ref = zipfile.ZipFile(ZPath + '/' + Dataset_name + '_' + i + '_data.zip', 'r') \n",
    "    zip_ref.extractall(dataset_path + '/' + i + '/' + Dataset_name)\n",
    "    zip_ref.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating HDF5 files from shuffled addrs lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "# import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from YNet_scripts import * ## importing scripts - currently backprop is defined outside this notebook.\n",
    "import tables\n",
    "from __future__ import division\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "# plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "np.random.seed(1) #reproducible randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating HDF5 file - can create formats for either tf or th frameworks. \n",
    "\n",
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "img_dtype = tables.UInt16Atom()  # dtype in which the images will be saved\n",
    "\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    data_shape = (0, 2, 200, 200)\n",
    "elif data_order == 'tf':\n",
    "    data_shape = (0, 200, 200, 2)\n",
    "    \n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "train_storage = hdf5_file.create_earray(hdf5_file.root, 'train_img', img_dtype, shape=data_shape)\n",
    "val_storage = hdf5_file.create_earray(hdf5_file.root, 'val_img', img_dtype, shape=data_shape)\n",
    "test_storage = hdf5_file.create_earray(hdf5_file.root, 'test_img', img_dtype, shape=data_shape)\n",
    "mean_storage = hdf5_file.create_earray(hdf5_file.root, 'train_mean', img_dtype, shape=data_shape)\n",
    "\n",
    "# create the label arrays and copy the labels data in them\n",
    "hdf5_file.create_array(hdf5_file.root, 'train_labels', train_labels)\n",
    "hdf5_file.create_array(hdf5_file.root, 'val_labels', val_labels)\n",
    "hdf5_file.create_array(hdf5_file.root, 'test_labels', test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the dataset.hdf5 file in case you want to recreate\n",
    "#hdf5_file.close()\n",
    "#print (data_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage import transform\n",
    "\n",
    "# a numpy array to save the mean of the images\n",
    "mean = np.zeros(data_shape[1:], np.float32)\n",
    "\n",
    "# loop over train addresses\n",
    "for i in range(len(train_addrs)):\n",
    "    \n",
    "    # print how many images are saved every 100 images\n",
    "    if i % 100 == 0 and i > 1:\n",
    "        print 'Train data: {}/{}'.format(i, len(train_addrs))\n",
    "        \n",
    "    # read an image and resize to (2,64, 64)\n",
    "    addr = train_addrs[i]\n",
    "    img = io.imread(addr)\n",
    "    #img = transform.resize(img, (2,64,64)) #NOTE: currently resizing images is done in Section_2\n",
    "    \n",
    "    # Any first-line image pre-processing could be done here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'tf':\n",
    "        img = np.moveaxis(img, 0,-1) #Check the reorder condition - it has nothing to do with tensorflow or theano atm. \n",
    "        \n",
    "    # save the image and calculate the mean so far\n",
    "    train_storage.append(img[None])\n",
    "    mean += img / float(len(train_labels))\n",
    "    \n",
    "# loop over validation addresses\n",
    "for i in range(len(val_addrs)):\n",
    "    \n",
    "    # print how many images are saved every 100 images\n",
    "    if i % 100 == 0 and i > 1:\n",
    "        print 'Validation data: {}/{}'.format(i, len(val_addrs))\n",
    "        \n",
    "    # read an image and resize to (2,64,64)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = val_addrs[i]\n",
    "    img = io.imread(addr)\n",
    "    #img = transform.resize(img, (2,64, 64)) #NOTE: currently resizing images is done in Section_2\n",
    "    \n",
    "    # Any first-line image pre-processing could be done here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'tf':\n",
    "        img = np.moveaxis(img, 0,-1)\n",
    "        \n",
    "    # save the image\n",
    "    val_storage.append(img[None])\n",
    "    \n",
    "# loop over test addresses\n",
    "for i in range(len(test_addrs)):\n",
    "    \n",
    "    # print how many images are saved every 100 images\n",
    "    if i % 100 == 0 and i > 1:\n",
    "        print 'Test data: {}/{}'.format(i, len(test_addrs))\n",
    "        \n",
    "    # read an image and resize to (2,64,64)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = test_addrs[i]\n",
    "    img = io.imread(addr)\n",
    "    #img = transform.resize(img, (2,64,64)) #NOTE: currently resizing images is done in Section_2\n",
    "    \n",
    "    # Any first-line image pre-processing could be done here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'tf':\n",
    "        img = np.moveaxis(img, 0,-1)\n",
    "        \n",
    "    # save the image\n",
    "    test_storage.append(img[None])\n",
    "    \n",
    "# save the mean and close the hdf5 file\n",
    "mean_storage.append(mean[None])\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing images from HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_yeast():\n",
    "    \n",
    "    yeast_dataset = tables.open_file('Yeast_ML_EXP1/dataset.hdf5', \"r\")\n",
    "    train_set_x_orig = np.array(yeast_dataset.root.train_img) # our train set features\n",
    "    train_set_y_orig = np.array(yeast_dataset.root.train_labels) # our train set labels\n",
    "\n",
    "    test_set_x_orig = np.array(yeast_dataset.root.test_img) # our test set features\n",
    "    test_set_y_orig = np.array(yeast_dataset.root.test_labels) # our test set labels\n",
    "\n",
    "    #classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    classes = np.array(['WT','Mfb1KO']) #hardcoded for now\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data_yeast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x_orig.shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
