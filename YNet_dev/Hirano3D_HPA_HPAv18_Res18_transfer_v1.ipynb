{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from resources.conv_learner import *\n",
    "from resources.plots import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "# data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV generation via sklearn, Multilabel implementation by trent-b, or FastAi\n",
    "\n",
    "Both libraries seem to be completely useless... can just use native fastai function: get_cv_idxs().<br>\n",
    "However, get_cv_idxs() does NOT shuffle...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def get_label_stratified_CV_idxs(csv_path):\n",
    "    \n",
    "    _all_labels = pd.read_csv(csv_path)\n",
    "    arr = _all_labels.values\n",
    "\n",
    "    X = arr[:,0]\n",
    "    y = arr[:,1:]\n",
    "    \n",
    "    ### sklearn.model_selection.StratifiedKFold\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "    \n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{pd.Series(arr[:,1][trn_idxs]).value_counts()}\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{pd.Series(arr[:,1][val_idxs]).value_counts()}\"\"\")\n",
    "    \n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "def get_label_stratified_CV_idxs_multi(csv_path):\n",
    "    \n",
    "    # FastAi csv_source expects a folder-name string to be passed as first arg... -> 'dummy'\n",
    "    X, y, all_lbls = csv_source('dummy', csv_path)\n",
    "    \n",
    "    ### Iterative stratification library: https://github.com/trent-b/iterative-stratification\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    \n",
    "    for train_index, val_index in msss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "    \n",
    "    trn_count = np.sum(y[trn_idxs], axis=0)\n",
    "    val_count = np.sum(y[val_idxs], axis=0)\n",
    "    \n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trn_count}\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{val_count}\"\"\")\n",
    "    \n",
    "    return trn_idxs, val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_path = path + 'Kaggle_AND_HPAv18_60x_NoDefDupes_labels.csv'\n",
    "trn_idxs, val_idxs = get_label_stratified_CV_idxs_multi(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# idxs_dict = {'trn_idxs': trn_idxs, 'val_idxs': val_idxs}\n",
    "\n",
    "# with open('datasets/HPA_challenge_2018/tmp/dn121_AWS_1_cont_idxs.pkl', 'wb') as handle:\n",
    "#     pickle.dump(idxs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('datasets/HPA_challenge_2018/tmp/dn121_AWS_1_cont_idxs.pkl', 'rb') as handle:\n",
    "    idxs_dict_load = pickle.load(handle)\n",
    "    \n",
    "val_idxs_loaded = idxs_dict_load['val_idxs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, lbl_csv, val_idxs, sz, bs, aug_tfms, crp_sz):\n",
    "\n",
    "    tfms = tfms_with_IntNorm(sz, aug_tfms=aug_tfms, crop_type=CropType.RANDOM, crp_sz=crp_sz)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'data', lbl_csv, \n",
    "                                        val_idxs = val_idxs,\n",
    "                                        test_name='data/Kaggle_test_GBRY', \n",
    "                                        tfms=tfms, bs=bs, suffix = '.tiff', \n",
    "                                        balance=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"datasets/HPA_challenge_2018/\"\n",
    "# data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### max-size bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "bs = 64\n",
    "sz = [4,775,775]\n",
    "\n",
    "# define augmentations\n",
    "augs = [RandomDihedral()]\n",
    "\n",
    "crp_sz = 1360/2056\n",
    "# csv:\n",
    "lbl_csv = path + 'Kaggle_AND_HPAv18_60x_NoDefDupes_labels.csv'\n",
    "\n",
    "# initialize data object\n",
    "data = get_data(PATH, lbl_csv, val_idxs_loaded, sz, bs, aug_tfms = augs, crp_sz=crp_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inpsect data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.trn_dl))\n",
    "im = to_np(x)[0]\n",
    "im.shape\n",
    "# x_test, y_test = next(iter(data.test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print transformations\n",
    "# plt.style.use('seaborn-white')\n",
    "\n",
    "c = 0\n",
    "idx = 0\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    x, y = next(iter(data.aug_dl))\n",
    "    im = to_np(x)[idx]\n",
    "#     ax.imshow(np.sum(im, axis = 0))\n",
    "    ax.imshow(im[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading epoch for manual inspection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epoch(dl = data.trn_dl):\n",
    "    \n",
    "    batch = iter(dl)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "#     for b in range(0,len(dl)):\n",
    "    for b in range(0,10):\n",
    "\n",
    "        x_, y_ = next(batch)\n",
    "        x_np, y_np = to_np(x_), to_np(y_)\n",
    "        xs.append(x_np)\n",
    "        ys.append(y_np)\n",
    "\n",
    "    return np.vstack(xs), np.concatenate(ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inpsecting loaded images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(y):\n",
    "    ind = [i for i, p in enumerate(y) if y[i]==1]\n",
    "    return(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect train images\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "idx = 2\n",
    "\n",
    "im = to_np(x)[idx]\n",
    "\n",
    "# lbl = to_np(y)[idx]\n",
    "lbl = to_label(to_np(y)[idx])\n",
    "print(lbl)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    ax.imshow(im[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect test images\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "idx = 3\n",
    "\n",
    "im = to_np(x_test)[idx]\n",
    "\n",
    "# lbl = to_label(to_np(y)[idx])\n",
    "# print(lbl)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    ax.imshow(im[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in learn.models.model.state_dict():\n",
    "    print(param_tensor, \"\\t\", learn.models.model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def check_model_loading(learn_obj, loaded_state_dict):\n",
    "    learn_weight0 = list(copy.deepcopy(learn_obj.models.model.state_dict()).items())[0] # need deepcopy here\n",
    "    learn_weight10 = list(copy.deepcopy(learn_obj.models.model.state_dict()).items())[604] # need deepcopy here\n",
    "\n",
    "    learn_obj.models.model.load_state_dict(loaded_state_dict, strict=False)\n",
    "    learn_weight0_loaded = list(learn_obj.models.model.state_dict().items())[0]\n",
    "    learn_weight10_loaded = list(learn_obj.models.model.state_dict().items())[604]\n",
    "\n",
    "#     print('weights_0:')\n",
    "#     print(learn_weight0[1][0] - learn_weight0_loaded[1][0])\n",
    "#     print('weights_10:')\n",
    "#     print(learn_weight10[1][0] - learn_weight10_loaded[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state_dict_path = 'datasets/Hirano3D/models/HPAv18_bs64_sz256_rCrp_dn121_v2_2_val_009.h5'\n",
    "loaded_state_dict = torch.load(saved_state_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncating saved model_state_dict()\n",
    "\n",
    "loaded_state_dict_trunc = copy.deepcopy(loaded_state_dict)\n",
    "del_keys = list(loaded_state_dict_trunc.keys())[604:]\n",
    "for key in del_keys: del loaded_state_dict_trunc[key]\n",
    "\n",
    "print('loaded_state_dict # keys: ', len(loaded_state_dict.keys()))    \n",
    "print('loaded_state_dict_trunc # keys: ', len(loaded_state_dict_trunc.keys()))    \n",
    "\n",
    "# Print model's state_dict\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in learn.models.model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", learn.models.model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet18_c\n",
    "\n",
    "learn = ConvLearner.pretrained(arch, data, opt_fn=optim.Adam, ps=0.5, pretrained=False)\n",
    "check_model_loading(learn, loaded_state_dict_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional parameters\n",
    "\n",
    "wd=1e-5 # weight-decay/L2 regularization \n",
    "learn.metrics = [accuracy_thresh(0.5),f1_macro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([1e-6,1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(lrs, 2, wds=wd, use_wd_sched=True, best_save_name='Hirano3D_dn121_local_8bit_transfHPA_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(lrs, 2, wds=wd, use_wd_sched=True, best_save_name='Hirano3D_dn121_local_8bit_transfHPA_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-4, 4, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.predict_with_targs()\n",
    "preds = np.argmax(log_preds, axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_preds, y =  learn.TTA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "plt.style.use('seaborn-white')\n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "cm = confusion_matrix(preds,y)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y =  learn.TTA(is_test=True)\n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be called by register_forward_hook\n",
    "\n",
    "def get_embeddings(layer_name):\n",
    "    def register_hook(layer_name):\n",
    "        def get_embedding(layer, inp, outp):\n",
    "            tmp = inp[0]\n",
    "            embedding.append(tmp)\n",
    "\n",
    "        hook = layer.register_forward_hook(get_embedding) \n",
    "        \n",
    "        for i in ['trn', 'val', 'test']:\n",
    "            embedding = []\n",
    "            preds, y = learn.predict_with_targs(i)\n",
    "            \n",
    "            # populating dict, consiting of [0]: preds, [1]: y, [2]: activations[layer]\n",
    "            embeddings[i] = [preds, y, np.vstack(to_np(embedding))]\n",
    "            \n",
    "        hook.remove()\n",
    "        \n",
    "    embeddings = {}    \n",
    "    layer = learn.models.model._modules.get(layer_name)\n",
    "    register_hook(layer)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls datasets/HPA_challenge_2018/tmp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('datasets/HPA_challenge_2018/tmp/Embeddings_HPAv18_bs64_sz256_rCrp_dn121_v2_2_val_009.pkl', 'wb') as handle:\n",
    "    pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('datasets/HPA_challenge_2018/tmp/Embeddings_HPAv18_bs64_sz256_rCrp_dn121_v2_2_val_009.pkl', 'rb') as handle:\n",
    "    embeddings_loaded = pickle.load(handle)\n",
    "    \n",
    "# embeddings = embeddings_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking embeddings\n",
    "\n",
    "embs_trn = embeddings['trn'][2]\n",
    "y_trn = embeddings['trn'][1]\n",
    "\n",
    "embs_val = embeddings['val'][2]\n",
    "y_val = embeddings['val'][1]\n",
    "\n",
    "embs_test = embeddings['test'][2]\n",
    "y_test = embeddings['test'][1]\n",
    "\n",
    "print(embs_trn.shape)\n",
    "print(embs_val.shape)\n",
    "print(embs_test.shape)\n",
    "\n",
    "print(y_trn.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_trn_embedding = umap.UMAP(n_neighbors=10,\n",
    "                      min_dist=0.3,\n",
    "                      metric='correlation').fit(embs_trn)\n",
    "\n",
    "UMAP_trn = UMAP_trn_embedding.embedding_\n",
    "UMAP_val = UMAP_trn_embedding.transform(embs_val)\n",
    "UMAP_test = UMAP_trn_embedding.transform(embs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(60)\n",
    "y_test[:30] = y_test[:30] +1 \n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting PCA vs TSNE results\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "compA = 0\n",
    "compB = 1\n",
    "\n",
    "for i in range(4):\n",
    "    trn_UMAP_cls = UMAP_trn[y_trn == i]\n",
    "\n",
    "    axarr[0].scatter(trn_UMAP_cls[:,compA], trn_UMAP_cls[:,compB], label = data.classes[i], s = 5)\n",
    "    axarr[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    if i == 0:\n",
    "        axarr[1].scatter(trn_UMAP_cls[:,compA], trn_UMAP_cls[:,compB], label = data.classes[i], s = 5)\n",
    "\n",
    "    axarr[0].set_xlim(-10,5)\n",
    "    axarr[0].set_ylim(-5,8)\n",
    "    \n",
    "for i in [0,1]:\n",
    "    test_UMAP_cls = UMAP_test[y_test == i]\n",
    "    \n",
    "    axarr[1].scatter(test_UMAP_cls[:,compA], test_UMAP_cls[:,compB], s = 5)\n",
    "    axarr[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axarr[1].set_xlim(-10,5)\n",
    "    axarr[1].set_ylim(-5,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_th(preds, targs, start=0.2, end=0.6, step=0.05):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [f1_macro(preds, targs, thresh=th, kind='macro') for th in ths]\n",
    "    idx = np.argmax(res)\n",
    "    return ths[idx], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train predictions\n",
    "\n",
    "preds_trn, targs_trn =  learn.predict_with_targs('trn')\n",
    "\n",
    "preds_trn_torch = torch.from_numpy(preds_trn)\n",
    "targs_trn_torch = torch.from_numpy(targs_trn)\n",
    "\n",
    "opt_th(preds_trn_torch, targs_trn_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_val_torch[0])\n",
    "print(preds_trn_torch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get val predictions\n",
    "\n",
    "preds_val, targs_val =  learn.predict_with_targs('val')\n",
    "\n",
    "preds_val_torch = torch.from_numpy(preds_val)\n",
    "targs_val_torch = torch.from_numpy(targs_val)\n",
    "\n",
    "opt_th(preds_val_torch, targs_val_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get val predictions\n",
    "\n",
    "preds_val, targs_val =  learn.TTA(8)\n",
    "preds_val= np.mean(preds_val, axis=0)\n",
    "preds_val_torch = torch.from_numpy(preds_val)\n",
    "targs_val_torch = torch.from_numpy(targs_val)\n",
    "\n",
    "opt_th(preds_val_torch, targs_val_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions\n",
    "\n",
    "preds_test, targs_test =  learn.predict_with_targs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs_test[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epoch(dl = data.trn_dl):\n",
    "    \n",
    "    batch = iter(dl)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "#     for b in range(0,len(dl)):\n",
    "    for b in range(0,10):\n",
    "\n",
    "        x_, y_ = next(batch)\n",
    "        x_np, y_np = to_np(x_), to_np(y_)\n",
    "        xs.append(x_np)\n",
    "        ys.append(y_np)\n",
    "\n",
    "    return np.vstack(xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_dl.sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = data.test_ds.fnames\n",
    "test_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(preds_file, output_name, th = 0.3, TTA=False):\n",
    "    \n",
    "    # creating submission file\n",
    "    \n",
    "    if TTA:\n",
    "        preds = preds_file.mean(axis=0)\n",
    "        print('TTA:',preds.shape)\n",
    "    else: preds = preds_file\n",
    "    \n",
    "    clss = np.arange(0, len(data.classes)) # get class indeces\n",
    "    res = np.array([' '.join(np.char.mod('%d', clss[np.where(p > th)])) for p in preds]) # generating output\n",
    "\n",
    "    # ensure that there are no empty cells: in case no value > thresh, fill in with argmax()\n",
    "    for i in range(res.shape[0]):\n",
    "        if res[i] == '':\n",
    "            res[i] = preds[i].argmax()\n",
    "\n",
    "    # getting image Ids\n",
    "    fnames = np.array([os.path.basename(im).split('.')[0] for im in data.test_ds.fnames])\n",
    "\n",
    "    # creating submission file\n",
    "    sub_df = pd.DataFrame(res, index=fnames, columns=['Predicted'])\n",
    "    sub_df.to_csv(output_name, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = PATH + 'submissions/Res18_pre_0.42_t-03.csv'\n",
    "\n",
    "create_submission(log_preds, submission_name, th=0.3, TTA=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label identities:\n",
    "\n",
    "cell_location_label = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# thresholds = np.linspace(0, 1, 1000)\n",
    "# score = 0.0\n",
    "# test_threshold=0.5*np.ones(28)\n",
    "# best_threshold=np.zeros(28)\n",
    "# best_val = np.zeros(28)\n",
    "# for i in range(28):\n",
    "#     for threshold in thresholds:\n",
    "#         test_threshold[i] = threshold\n",
    "#         max_val = np.max(preds_y)\n",
    "#         val_predict = (preds_y > test_threshold)\n",
    "#         score = f1_score(valid_y > 0.5, val_predict, average='macro')\n",
    "#         if score > best_val[i]:\n",
    "#             best_threshold[i] = threshold\n",
    "#             best_val[i] = score\n",
    "#     print(\"Threshold[%d] %0.6f, F1: %0.6f\" % (i,best_threshold[i],best_val[i]))\n",
    "#     test_threshold[i] = best_threshold[i]\n",
    "# print(\"Best threshold: \")\n",
    "# print(best_threshold)\n",
    "# print(\"Best f1:\")\n",
    "# print(best_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
