{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from resources.conv_learner import *\n",
    "from resources.plots import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"datasets/yeast_v10.5/\"\n",
    "data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ('WT', 'mfb1KO', 'mmr1KO', 'dnm1KO', 'LatA_5uM', 'fzo1KO', 'CK666')\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating normalization statistics\n",
    "\n",
    "Note that we are setting up train & val data, as well as test. Within test, we are here including a mutant cell type that the model never trains on. The idea is to use to the feature space developed during training to evaluate novel cell types by similarity to the landmarks that the model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stats_name = \"yeast_v10.2_per_class.dict\"\n",
    "classes = Statistics.source_class(data_path)\n",
    "\n",
    "train_val = zip(classes['train'], classes['val'])\n",
    "test_ = zip(classes['test'])\n",
    " \n",
    "main_stats = Statistics.per_class(train_val)\n",
    "test_stats = Statistics.per_class(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for keys in main_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {main_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for keys in test_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {test_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfms_for_test(stats, sz):\n",
    "    test_norm = Normalize(stats)\n",
    "    test_denorm = Denormalize(stats)\n",
    "    val_crop = CropType.NO\n",
    "    test_tfms = image_gen(test_norm, test_denorm,sz, crop_type=val_crop)\n",
    "    return test_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: str, sz, bs): # , num_workers=1\n",
    "    create, lbl2index, lbl2index_test = ImageClassifierData.prepare_from_path(path, val_name='val', bs=bs,\n",
    "                                                                             test_name='test', test_with_labels=True, balance = True)\n",
    "    \n",
    "    main_stats_X = {lbl2index[key][0]: val for key, val in main_stats.items()}\n",
    "    tfms = tfms_from_stats(main_stats_X, sz, aug_tfms=[RandomDihedral()], crop_type=CropType.CENTER, pad=sz//8)\n",
    "    \n",
    "    test_stats_X = {lbl2index_test[key][0]: val for key, val in test_stats.items()}\n",
    "    test_tfms = tfms_for_test(test_stats_X,sz)\n",
    "    tfms += (test_tfms, )\n",
    "    \n",
    "#     print(main_stats_X)\n",
    "#     print(test_stats_X)\n",
    "    \n",
    "#     print('\\n class to index mapping:\\n',lbl2index)\n",
    "#     print('\\n class to index mapping:\\n',lbl2index_test)\n",
    "    return create(tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = get_data(PATH,SIZE,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print transformations\n",
    "\n",
    "idx = 0 \n",
    "fig, ax = plt.subplots(2,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    x, y = next(iter(data.aug_dl))\n",
    "    im = to_np(x)[idx]\n",
    "    ax.imshow(np.sum(im, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.val_dl.dataset.y))\n",
    "print(len(data.trn_dl.dataset.y))\n",
    "print(len(data.test_dl.dataset.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect loaded data:\n",
    "\n",
    "Displaying the same image with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify which image-index\n",
    "idx = 0\n",
    "\n",
    "# loading it from GPU to CPU\n",
    "xx = x[idx].cpu().numpy().copy()\n",
    "yy = y[idx]\n",
    "# showing the image\n",
    "#\n",
    "#sp.axis('Off')\n",
    "#sp.set_title(\"Norm\", fontsize=11)\n",
    "figure, _ ,_ = tiff.imshow(np.sum(xx, axis=0))\n",
    "figure.set_size_inches(6,6)\n",
    "figure.add_subplot(111)\n",
    "\n",
    "# figure2, _, _ = tiff.imshow(np.sum(data.trn_ds.denorm(xx,yy).squeeze() * 65536, axis=2)) # not very elegant atm. \n",
    "# figure2.set_size_inches(6,6)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet_with_Batchnorm\n",
    "\n",
    "Defining network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False, padding=1)\n",
    "        self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds  = x_chan.std (1)[:,None,None]\n",
    "        return (x-self.means) / self.stds *self.m + self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 7), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 8, cycle_len=1, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.trn_dl.sampler#__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "x = learn.sched.plot_loss()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some more cylces - error & accuracy should continuously improve\n",
    "\n",
    "Note: cycle len = number of epochs per cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 4, wds=wd, cycle_len=5, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 4, wds=wd, cycle_len=5, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 2, wds=wd, cycle_len=5, cycle_mult = 2, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 2, wds=wd, cycle_len=3, cycle_mult=2, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 3, wds=wd, cycle_len=3, cycle_mult=2, use_clr=(20,8, 0.95, 0.85), best_save_name='YNet_v10.5_balanced_Rrotate(180)_Rflip_Rblur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(2e-2, 8, wds=wd, cycle_len=3, use_clr=(20,8, 0.95, 0.85), best_save_name='YNet_v10.5_balanced_Rrotate(180)_Rflip_Rblur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "74, 79, 83, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-1 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust = {0:20}\n",
    "adjust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-4, 8, wds=wd, cycle_len=1, adjust_class=adjust) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Model evaluation\n",
    "\n",
    "This is one of the major areas that needs improvement in our workflow. The tools we have so far (confusion matrix and manual inpsection of images) are essential but definitely not sufficient to ensure that our model learns something biologicaly relevant. Ideas are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('YNet_v10.5_balanced_Rrotate(180)_Rflip_Rblur_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model:\n",
    "learn.load('YNet_Res_v10.5_balanced_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.warm_up(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(n_aug=1) # run predictions with TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross_validation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "plt.style.use('seaborn-white')\n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "cm = confusion_matrix(preds,y)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-set eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.models.model._modules.get('out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be called by register_forward_hook\n",
    "\n",
    "def get_embeddings(layer, inp, outp):\n",
    "    \n",
    "#     in_dim = inp[0].shape[0]\n",
    "#     tmp = np.zeros((inp[0].shape[0], inp[0].shape[1]))\n",
    "    tmp = inp[0]\n",
    "    embeddings.append(tmp)\n",
    "#     print(inp[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting a target layer module:\n",
    "layer = learn.models.model._modules.get('out')\n",
    "\n",
    "# registering the hook\n",
    "hook = layer.register_forward_hook(get_embeddings)\n",
    "\n",
    "# running learn.predict and collecting all activations from the target layer\n",
    "embeddings = []\n",
    "log_preds, y = learn.predict_with_targs()\n",
    "\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.vstack(to_np(embeddings))\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running PCA on embeddings\n",
    "n_components = 50\n",
    "pca = PCA(n_components)\n",
    "pca_result = pca.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting compenent-wise cumulative explained variance (for PCA)\n",
    "\n",
    "plt.plot(range(n_components), pca.explained_variance_ratio_)\n",
    "plt.plot(range(n_components), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component-wise and Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# running TSNE on embeddings\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting PCA vs TSNE results\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "compA = 0\n",
    "compB = 1\n",
    "\n",
    "for i in range(7):\n",
    "    PCA_cls = pca_result[y == i]\n",
    "    TSNE_cls = tsne_results[y == i]\n",
    "    \n",
    "    axarr[0].scatter(PCA_cls[:,compA], PCA_cls[:,compB], label = data.classes[i])\n",
    "    axarr[1].scatter(TSNE_cls[:,compA], TSNE_cls[:,compB], label = data.classes[i])\n",
    "    axarr[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3D TSNE space from PCA of embeddings\n",
    "\n",
    "tsne3D = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300, learning_rate=50.0)\n",
    "PCA2tSNE_results_3D = tsne3D.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# magic command to make plots interactive.\n",
    "# restart kernel if you want to switch to e.g. %matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plotting - very slow interactivity atm...\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "compA = 0\n",
    "compB = 1\n",
    "compC = 2\n",
    "\n",
    "for i in range(10):\n",
    "    clss = PCA2tSNE_results_3D[y == i]\n",
    "    cls_C = clss[0:500,:]\n",
    "    ax.scatter3D(cls_C[:,compA], cls_C[:,compB], cls_C[:,compC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing train and test datasets as exposed by dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Yinan, please take the functionality of the next 2 cells and transfer it to the data_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_ = data.trn_dl\n",
    "batch_ = iter(dl_)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for b in range(0,len(dl_)):\n",
    "    x_, y_ = next(batch_)\n",
    "\n",
    "    x_np = to_np(x_)\n",
    "    y_np = to_np(y_)\n",
    "    \n",
    "    im_means = np.mean(x_np, axis=(2,3))\n",
    "    \n",
    "    ax.plot(im_means[:,0], im_means[:,1], 'o', color = 'C0' , alpha=0.5)\n",
    "        \n",
    "plt.xlim(-0.4,0.4)\n",
    "plt.ylim(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_t = data.test_dl\n",
    "batch_t = iter(dl_t)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for b in range(0,len(dl_t)):\n",
    "    x_, y_ = next(batch_t)\n",
    "\n",
    "    x_np = to_np(x_)\n",
    "    y_np = to_np(y_)\n",
    "    \n",
    "    im_means = np.mean(x_np, axis=(2,3))\n",
    "    \n",
    "    ax.plot(im_means[:,0], im_means[:,1], 'o', color = 'C1' , alpha=0.5)\n",
    "    \n",
    "plt.xlim(-0.4,0.4)\n",
    "plt.ylim(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_preds, targs = learn.predict_with_targs(is_test=True)\n",
    "testprobs = np.exp(test_log_preds)\n",
    "preds = np.argmax(testprobs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @James, please transfer the functionality of the next 5 cells into the data_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @James, there is definitely a simpler way of generating test_lbl2idx_ than calling this entire line. Please trim it down. \n",
    "\n",
    "_, lbl2idx_, test_lbl2idx_ = ImageClassifierData.prepare_from_path(PATH, val_name='val', bs=64, num_workers=1, test_name='test', test_with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions dictionary\n",
    "\n",
    "h = 0\n",
    "preds_dict = {}\n",
    "for i, key in enumerate(test_lbl2idx_.keys()):\n",
    "    l = h\n",
    "    h = h + list(data.test_dl.dataset.src_idx).count(i)\n",
    "    preds_dict[key] = list(preds[l:h])\n",
    "    print(f\"{key} predictions ready ({h - l} elements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict['03_bud1KO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rel = {}\n",
    "for key in preds_dict.keys():\n",
    "    print(key)\n",
    "    val = {cls: preds_dict[key].count(i)/len(preds_dict[key]) for i, cls in enumerate(data.classes)}\n",
    "    preds_rel[key]= val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_preds(targets, preds_rel):\n",
    "    \n",
    "    if not isinstance(targets, list):\n",
    "        targets = [targets]\n",
    "        \n",
    "    x = math.ceil((int(len(targets)) /2)) # dynamic scaling of GridSpec\n",
    "    sz = 4 * x # dynamic scaling of figuresize\n",
    "    \n",
    "    # plotting:\n",
    "    plt.figure(figsize=(12,sz))\n",
    "    gs1 = plt.GridSpec(x,2)\n",
    "    gs1.update(wspace = 0.4)\n",
    "\n",
    "    for i, targ in enumerate(targets):\n",
    "        to_plot = [preds_rel[targ][key] for key in data.classes] # extracting data\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.barh(data.classes, to_plot)\n",
    "        ax1.set_title(targ)\n",
    "        ax1.set_xlim(0,1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = list(test_lbl2idx_.keys())\n",
    "\n",
    "plot_test_preds(test_classes, preds_rel)\n",
    "# plot_test_preds(['01_WT', '03_WT', '03_fzo1KO', '01_mfb1KO'], preds_rel)\n",
    "# plot_test_preds(['01_WT'], preds_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show random correct/incorrectly classified images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds_mean = np.mean(log_preds, axis=0) # averages predictions on original + 4 TTA images\n",
    "preds = np.argmax(log_preds_mean, axis=1) # converts into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = np.exp(log_preds_mean[:,0]) # prediction(WT)\n",
    "probs = np.exp(log_preds_mean) # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\n",
    "def rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, channel, figsize=(12,6), rows=1, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None: sp.set_title(titles[i], fontsize=11)\n",
    "        if channel is not None: plt.imshow(ims[i,channel,:,:]) \n",
    "        else: plt.imshow(np.sum(ims, axis=1)[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_with_title_from_ds_no_denorm(idxs, title, channel=None):\n",
    "    \n",
    "    imgs = np.stack(data.val_ds[x][0] for x in idxs) # get images by idx\n",
    "    corr_lbl = np.stack(data.val_ds[x][1] for x in idxs) # get correct label from data.val_ds by idx\n",
    "    pred_lbl = np.stack(preds[x] for x in idxs) # get predicted label from preds by idx\n",
    "    p_max = [np.amax(probs[x,:]) for x in idxs] # get highest probability from probs by idx\n",
    "    \n",
    "    title_fin = [f\"true = {corr_lbl[x]}\\n predicted: {pred_lbl[x]}\\n  p = {p_max[x]}\" for x, _ in enumerate(corr_lbl)]\n",
    "    print(title)\n",
    "    \n",
    "    return plots(imgs, channel, rows=1, titles=title_fin, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot images according to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load from ds - not denormalized! \n",
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(True), \"Correctly classified\")\n",
    "#optionally pass channel arg. to select single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(False), \"Incorrectly classified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show most correct/incorrectly classified images per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_by_mask(mask, y, mult):\n",
    "    idxs = np.where(mask)[0]\n",
    "    return idxs[np.argsort(mult * probs[:,y][idxs])[0:5]]\n",
    "\n",
    "def most_by_correct(y, is_correct): \n",
    "    mult = -1 if is_correct else 1\n",
    "    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), y, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, True), \"Most correctly classified WT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, False), \"Most incorrectly classified WT\") # logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, True), \"Most correctly classified mfb1KO\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, False), \"Most incorrectly classified mfb1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(2, True), \"Most correctly classified mfb1KO-mmr1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(3, True), \"Most correctly classified CK666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(4, True), \"Most correctly classified LatA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(5, True), \"Most correctly classified dnnm1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(6, True), \"Most correctly classified fzo1KO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show (most) uncertain images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_uncertain = t = np.argsort(np.amax(probs, axis = 1))[1:5] # get best \"guess\" per image and list the least confident ones\n",
    "plot_val_with_title_from_ds_no_denorm(most_uncertain, \"Most uncertain predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl2idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
