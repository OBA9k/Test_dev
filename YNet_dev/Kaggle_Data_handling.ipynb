{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from pathlib import Path\n",
    "\n",
    "from resources.conv_learner import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "# path = \"datasets/Kagg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "Below are two functions which: \n",
    "\n",
    "a) select from the original csv a certain percentage of images at random and save them in a second csv<br/>\n",
    "b) based on an input csv, select image files, compose stacks of images according to selected channels and save them in a separate folder as tiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_subset_csv(path, infile, outfile, percentage = 10, seed=None):\n",
    "\n",
    "    perc = math.ceil((int(100 / percentage)))\n",
    "    if seed: random.seed = seed # sharing outfiles directly might be easier\n",
    "    rand_subset = []\n",
    "    print(f\"Generating random {perc}% subset of {infile}...\")\n",
    "    \n",
    "    with open(path + infile, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        train_data = np.vstack(list(reader))\n",
    "\n",
    "    for idx, file in enumerate(train_data[:,0]):\n",
    "        number = random.randint(1,perc)\n",
    "        if number == 1:\n",
    "            rand_subset.append(train_data[idx])\n",
    "    \n",
    "    random_subset = np.vstack(rand_subset)\n",
    "    \n",
    "    # write to csv in the same directory:\n",
    "    # some error-catching might be nice if outfile already exists\n",
    "    df = pd.DataFrame(random_subset)    \n",
    "    df.to_csv(path + outfile, header = False, index = False)\n",
    "    print(f\"Success: saved random {perc}% subset to {outfile}\")\n",
    "    \n",
    "    return random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random 10% subset of train.csv...\n",
      "Success: saved random 10% subset to train_10perc_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# generate random subset csv file:\n",
    "\n",
    "random_ten_percent = generate_random_subset_csv(path, 'train.csv', 'train_10perc_v2.csv', seed = 1)\n",
    "# print(random_ten_percent[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for consistency between subset and source\n",
    "\n",
    "a = list(train_data[:,0])\n",
    "b = list(random_ten_percent[:,0])\n",
    "\n",
    "c = set(a) & set(b)\n",
    "assert len(b) == len(c), \"Selected images not subset of original data\"\n",
    "print('Check successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset \n",
    "\n",
    "dataset_gen():\n",
    "\n",
    "1) Generates composite images (if multiple channels have been selected) from the individual blue, green, yellow and red channels of the original images.<br/>\n",
    "2) The input csv defines the exact images to be selected.<br/>\n",
    "3) Images are stored in a newly generated folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_gen(csv_path, source_path, output_path, channels = ['green']):\n",
    "    \n",
    "    with open(csv_path, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        fnames = np.vstack(list(reader))[1:,0] #### change depending on header ###\n",
    "    \n",
    "    assert not os.path.isdir(output_path), 'Chosen output directory already exists!'\n",
    "    print(f\"creating output directory: {os.path.basename(output_path)}\")\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    for f in tqdm(fnames, total = len(fnames), unit=\"files\"):\n",
    "        im_ = [cv2.imread(str(source_path + f + '_' + c + '.png'), cv2.IMREAD_GRAYSCALE) for c in channels] # check '_' or '-'\n",
    "        im = np.stack(im_)\n",
    "        \n",
    "        assert im.dtype == 'uint8' # making sure files are expected dtype; may change for different datasets...\n",
    "        tiff.imsave(output_path + f + '.tiff', im, imagej = True) # imageJ = True generates ImageJ compatible Hyperstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating output directory: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3423217844f48088fd5e9c9e00cb777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "\n",
    "csv_path = path + 'HPA_labels.csv'\n",
    "source_path = path + 'train_raw/'\n",
    "output_path = path + 'Kaggle_train_GBRY/'\n",
    "channels = ['green','blue','red','yellow'] # can also leave this out since default is 'green'\n",
    "\n",
    "dataset_gen(csv_path, source_path, output_path, channels = channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/ -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf datasets/HPA_challenge_2018/Kaggle_train_GBRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPA_labels.csv\t\t\t       haystack_no_si.csv\r\n",
      "HPAv18RBGY_wodpl.csv\t\t       models\r\n",
      "HPAv18_60x_def_dupes_removed.csv       needles.csv\r\n",
      "HPAv18_BGRY_source\t\t       sample_submission.csv\r\n",
      "HPAv18_BGR_all\t\t\t       test-raw\r\n",
      "HPAv18_BGR_source\t\t       test.zip\r\n",
      "HPAv18_GBRY_60x_def_dupes_removed_all  test_BGR_all\r\n",
      "HPAv18_def_dupes_removed.csv\t       test_BGYR_all\r\n",
      "HPAv18_dupes_400.csv\t\t       tmp\r\n",
      "HPAv18_dupes_5k.csv\t\t       train.csv\r\n",
      "HPAv18_dupes_no_si_dist5.csv\t       train.zip\r\n",
      "HPAv18_dupes_no_si_dist5.pkl\t       train_BGR_all\r\n",
      "HPAv18_dupes_no_si_phash_10_BGR.pkl    train_BGYR_all\r\n",
      "HPAv18_wodpl_60x.csv\t\t       train_raw\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = os.listdir('datasets/HPA_challenge_2018/HPAv18_BGR_all')\n",
    "tiff.imread('datasets/HPA_challenge_2018/HPAv18_BGR_all/' + d[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512, 512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf datasets/HPA_challenge_2018/HPAv18_BGR_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create non-header version of input .csv\n",
    "\n",
    "# csv_path = path + 'HPA_labels.csv'\n",
    "\n",
    "\n",
    "# with open(csv_path, 'r') as d:\n",
    "#     reader = csv.reader(d)\n",
    "#     fnames = np.vstack(list(reader))\n",
    "    \n",
    "# no_header = fnames[1:] # remove header line\n",
    "\n",
    "# no_header_df = pd.DataFrame(no_header)\n",
    "# no_header_df\n",
    "\n",
    "# no_header_df.to_csv(path + 'HPA_labels.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling test-set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_gen(path, test_folder, output_path, channels = ['green']):\n",
    "    \n",
    "    _fnames = test_names = read_dir(path, test_folder)\n",
    "    _fnames = pd.DataFrame([os.path.basename(_fnames[i].split('_')[0]) for i, _ in enumerate(_fnames)])\n",
    "    fnames = _fnames[0].unique();\n",
    "    \n",
    "    assert not os.path.isdir(output_path), 'Chosen output directory already exists!'\n",
    "    print(f\"creating output directory: {os.path.basename(output_path)}\")\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    for f in tqdm(fnames, total = len(fnames), unit=\"files\"):\n",
    "        im_ = [cv2.imread(str(path + test_folder + f + '_' + c + '.png'), cv2.IMREAD_GRAYSCALE) for c in channels]\n",
    "        im = np.stack(im_)\n",
    "        # im = np.rollaxis(im,0,3)\n",
    "\n",
    "        assert im.dtype == 'uint8' # making sure files are expected dtype; may change for different datasets...\n",
    "        tiff.imsave(output_path + f + '.tiff', im, imagej = True) # imageJ = True generates ImageJ compatible Hyperstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating output directory: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346150feab714f948619ed93f54e7538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11702), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "path = path\n",
    "test_folder = 'test-raw/'\n",
    "output_path = path + 'Kaggle_test_GBRY/'\n",
    "channels = ['green','blue','red','yellow'] # can also leave this out since default is 'green'\n",
    "\n",
    "testset_gen(path, test_folder, output_path, channels = channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multi-folder csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPA_labels.csv\t\t\t       Kaggle_train_GBRY\r\n",
      "HPAv18RBGY_wodpl.csv\t\t       haystack_no_si.csv\r\n",
      "HPAv18_60x_def_dupes_removed.csv       models\r\n",
      "HPAv18_BGRY_source\t\t       needles.csv\r\n",
      "HPAv18_BGR_all\t\t\t       sample_submission.csv\r\n",
      "HPAv18_BGR_source\t\t       test-raw\r\n",
      "HPAv18_GBRY_60x_def_dupes_removed_all  test.zip\r\n",
      "HPAv18_def_dupes_removed.csv\t       test_BGR_all\r\n",
      "HPAv18_dupes_400.csv\t\t       test_BGYR_all\r\n",
      "HPAv18_dupes_5k.csv\t\t       tmp\r\n",
      "HPAv18_dupes_no_si_dist5.csv\t       train.csv\r\n",
      "HPAv18_dupes_no_si_dist5.pkl\t       train.zip\r\n",
      "HPAv18_dupes_no_si_phash_10_BGR.pkl    train_BGR_all\r\n",
      "HPAv18_wodpl_60x.csv\t\t       train_BGYR_all\r\n",
      "Kaggle_test_GBRY\t\t       train_raw\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31G\tdatasets/HPA_challenge_2018/Kaggle_train_GBRY\n",
      "12G\tdatasets/HPA_challenge_2018/Kaggle_test_GBRY\n",
      "66G\tdatasets/HPA_challenge_2018/HPAv18_GBRY_60x_def_dupes_removed_all\n"
     ]
    }
   ],
   "source": [
    "!du -h datasets/HPA_challenge_2018/Kaggle_train_GBRY\n",
    "!du -h datasets/HPA_challenge_2018/Kaggle_test_GBRY\n",
    "!du -h datasets/HPA_challenge_2018/HPAv18_GBRY_60x_def_dupes_removed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Kaggle_train_GBRY: 31071\n",
      "Files in Kaggle_test_GBRY: 11702\n",
      "Files in HPAv18_GBRY_60x_def_dupes_removed_all: 66921\n",
      "Image-dims in Kaggle_train_GBRY: (4, 512, 512)\n",
      "Image-dims in Kaggle_test_GBRY: (4, 512, 512)\n",
      "Image-dims in HPAv18_GBRY_60x_def_dupes_removed_all: (4, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Files in Kaggle_train_GBRY: {len(os.listdir('datasets/HPA_challenge_2018/Kaggle_train_GBRY'))}\")\n",
    "print(f\"Files in Kaggle_test_GBRY: {len(os.listdir('datasets/HPA_challenge_2018/Kaggle_test_GBRY'))}\")\n",
    "print(f\"Files in HPAv18_GBRY_60x_def_dupes_removed_all: {len(os.listdir('datasets/HPA_challenge_2018/HPAv18_GBRY_60x_def_dupes_removed_all'))}\")\n",
    "      \n",
    "      \n",
    "print(f\"Image-dims in Kaggle_train_GBRY: {tiff.imread(path + 'Kaggle_train_GBRY/' + os.listdir('datasets/HPA_challenge_2018/Kaggle_train_GBRY')[0]).shape}\")\n",
    "print(f\"Image-dims in Kaggle_test_GBRY: {tiff.imread(path + 'Kaggle_test_GBRY/' + os.listdir('datasets/HPA_challenge_2018/Kaggle_test_GBRY')[0]).shape}\")\n",
    "print(f\"Image-dims in HPAv18_GBRY_60x_def_dupes_removed_all: {tiff.imread(path + 'HPAv18_GBRY_60x_def_dupes_removed_all/' + os.listdir('datasets/HPA_challenge_2018/HPAv18_GBRY_60x_def_dupes_removed_all')[0]).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add partial path to Id's in csv's\n",
    "\n",
    "def multi_folder_csv(df_folderpath_dict):\n",
    "    \"\"\"\n",
    "    df_folderpath_dict: dictionary of (at least 2 pairs) of {Partial_folder_path : label_dataframe}, \n",
    "    e.g. with label_dataframes containing columns 'Id' and 'Target'.\n",
    "    \n",
    "    res: one unified pandas DataFrame with Partial_folder_path added to 'Id's \n",
    "    \"\"\"\n",
    "    _dict = df_folderpath_dict.copy()\n",
    "    for partial_path, df in _dict.items():\n",
    "        for n in tqdm(df.Id, total = len(df.Id), unit=\"files\"):\n",
    "            df.Id.replace(n, partial_path + n, inplace=True)  \n",
    "            \n",
    "    res = list(_dict.values())[0].append(list(_dict.values())[1:])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPA_labels.csv\t\t\t       Kaggle_train_GBRY\r\n",
      "HPAv18RBGY_wodpl.csv\t\t       haystack_no_si.csv\r\n",
      "HPAv18_60x_def_dupes_removed.csv       models\r\n",
      "HPAv18_BGRY_source\t\t       needles.csv\r\n",
      "HPAv18_BGR_all\t\t\t       sample_submission.csv\r\n",
      "HPAv18_BGR_source\t\t       test-raw\r\n",
      "HPAv18_GBRY_60x_def_dupes_removed_all  test.zip\r\n",
      "HPAv18_def_dupes_removed.csv\t       test_BGR_all\r\n",
      "HPAv18_dupes_400.csv\t\t       test_BGYR_all\r\n",
      "HPAv18_dupes_5k.csv\t\t       tmp\r\n",
      "HPAv18_dupes_no_si_dist5.csv\t       train.csv\r\n",
      "HPAv18_dupes_no_si_dist5.pkl\t       train.zip\r\n",
      "HPAv18_dupes_no_si_phash_10_BGR.pkl    train_BGR_all\r\n",
      "HPAv18_wodpl_60x.csv\t\t       train_BGYR_all\r\n",
      "Kaggle_test_GBRY\t\t       train_raw\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm datasets/HPA_challenge_2018/Kaggle_HPA_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPAv18_labels_df = pd.read_csv(path + 'HPA_labels.csv')\n",
    "HPAv18_labels_df = pd.DataFrame(HPAv18_labels_df.values, columns=['Id', 'Target'])\n",
    "HPAv18_labels_df.to_csv(path + 'Kaggle_HPA_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "HPA_labels_df = pd.read_csv(path + 'Kaggle_HPA_labels.csv')\n",
    "HPAv18_labels_df = pd.read_csv(path + 'HPAv18_60x_def_dupes_removed_labels.csv')\n",
    "\n",
    "df_folderpath_dict = {'Kaggle_train_GBRY/':HPA_labels_df,\n",
    "                     'HPAv18_GBRY_60x_def_dupes_removed_all/': HPAv18_labels_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1830b66a5e874a2ca65f919b6aba7eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b70721eae0c40a6976986cc0bda6654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=66921), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mod_df = multi_folder_csv(df_folderpath_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaggle_train_GBRY/000a6c98-bb9b-11e8-b2b9-ac1f...</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kaggle_train_GBRY/000a9596-bbc4-11e8-b2bc-ac1f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kaggle_train_GBRY/000c99ba-bba4-11e8-b2b9-ac1f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaggle_train_GBRY/001838f8-bbca-11e8-b2bc-ac1f...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kaggle_train_GBRY/001bcdd2-bbb2-11e8-b2ba-ac1f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kaggle_train_GBRY/0020af02-bbba-11e8-b2ba-ac1f...</td>\n",
       "      <td>25 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaggle_train_GBRY/002679c2-bbb6-11e8-b2ba-ac1f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kaggle_train_GBRY/00285ce4-bba0-11e8-b2b9-ac1f...</td>\n",
       "      <td>2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kaggle_train_GBRY/002daad6-bbc9-11e8-b2bc-ac1f...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kaggle_train_GBRY/002ff91e-bbb8-11e8-b2ba-ac1f...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id   Target\n",
       "0  Kaggle_train_GBRY/000a6c98-bb9b-11e8-b2b9-ac1f...  7 1 2 0\n",
       "1  Kaggle_train_GBRY/000a9596-bbc4-11e8-b2bc-ac1f...        5\n",
       "2  Kaggle_train_GBRY/000c99ba-bba4-11e8-b2b9-ac1f...        1\n",
       "3  Kaggle_train_GBRY/001838f8-bbca-11e8-b2bc-ac1f...       18\n",
       "4  Kaggle_train_GBRY/001bcdd2-bbb2-11e8-b2ba-ac1f...        0\n",
       "5  Kaggle_train_GBRY/0020af02-bbba-11e8-b2ba-ac1f...     25 2\n",
       "6  Kaggle_train_GBRY/002679c2-bbb6-11e8-b2ba-ac1f...        0\n",
       "7  Kaggle_train_GBRY/00285ce4-bba0-11e8-b2b9-ac1f...      2 0\n",
       "8  Kaggle_train_GBRY/002daad6-bbc9-11e8-b2bc-ac1f...        7\n",
       "9  Kaggle_train_GBRY/002ff91e-bbb8-11e8-b2ba-ac1f...       23"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_df.to_csv(path + 'Kaggle_AND_HPAv18_60x_NoDefDupes_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources WIP/Code storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast = tiff.imread(str('datasets/yeast_v11.1/train/02_WT/WT_WP_E2_Mito0_S2_F1_I1_C1_A0.tifstack.tif'))\n",
    "yeast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"ImageJ=1.52b\n",
    "# images=2\n",
    "# slices=2\n",
    "# unit=micron\n",
    "# spacing=0.3\n",
    "# loop=false\n",
    "# min=0.0\n",
    "# max=65535.0\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
