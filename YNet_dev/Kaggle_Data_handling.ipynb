{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "Below are two functions which: \n",
    "\n",
    "a) select from the original csv a certain percentage of images at random and save them in a second csv<br/>\n",
    "b) based on an input csv, select image files, compose stacks of images according to selected channels and save them in a separate folder as tiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"datasets/Kaggle_HPA_2018/\"\n",
    "path = \"G:/Downloads/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_subset_csv(path, infile, outfile, percentage = 10, seed=None):\n",
    "\n",
    "    perc = math.ceil((int(100 / percentage)))\n",
    "    if seed: random.seed = seed # sharing outfiles directly might be easier\n",
    "    rand_subset = []\n",
    "    print(f\"Generating random {perc}% subset of {infile}...\")\n",
    "    \n",
    "    with open(path + infile, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        train_data = np.vstack(list(reader))\n",
    "\n",
    "    for idx, file in enumerate(train_data[:,0]):\n",
    "        number = random.randint(1,perc)\n",
    "        if number == 1:\n",
    "            rand_subset.append(train_data[idx])\n",
    "    \n",
    "    random_subset = np.vstack(rand_subset)\n",
    "    \n",
    "    # write to csv in the same directory:\n",
    "    # some error-catching might be nice if outfile already exists\n",
    "    df = pd.DataFrame(random_subset)    \n",
    "    df.to_csv(path + outfile, header = False, index = False)\n",
    "    print(f\"Success: saved random {perc}% subset to {outfile}\")\n",
    "    \n",
    "    return random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random 10% subset of train.csv...\n",
      "Success: saved random 10% subset to test.csv\n"
     ]
    }
   ],
   "source": [
    "# generate random subset csv file:\n",
    "\n",
    "random_ten_percent = generate_random_subset_csv(path, 'train.csv', 'test.csv', seed = 1)\n",
    "# print(random_ten_percent[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6dae004dcca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check for consistency between subset and source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_ten_percent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for consistency between subset and source\n",
    "\n",
    "a = list(train_data[:,0])\n",
    "b = list(random_ten_percent[:,0])\n",
    "\n",
    "c = set(a) & set(b)\n",
    "assert len(b) == len(c), \"Selected images not subset of original data\"\n",
    "print('Check successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_gen(csv_path, source_path, output_path, channels = ['green']):\n",
    "    \n",
    "    with open(csv_path, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        fnames = np.vstack(list(reader))[:,0]\n",
    "    \n",
    "    assert not os.path.isdir(output_path), 'Chosen output directory already exists!'\n",
    "    print(f\"creating output directory: {os.path.basename(output_path)}\")\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    for f in tqdm(fnames[:10], total = len(fnames[:10]), unit=\"files\"):\n",
    "        im_ = [cv2.imread(str(source_path + f + '_' + c + '.png'), cv2.IMREAD_GRAYSCALE) for c in channels]\n",
    "        im = np.stack(im_)\n",
    "        # im = np.rollaxis(im,0,3)\n",
    "\n",
    "        assert im.dtype == 'uint8' # making sure files are expected dtype; may change for different datasets...\n",
    "        tiff.imsave(output_path + f + '.tiff', im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "csv_path = path + 'train_10perc_v1.csv'\n",
    "source_path = path + 'train_original/'\n",
    "output_path = path + 'train_4chan_all/'\n",
    "channels = ['blue','green','yellow','red'] # can also leave this out since default is 'green'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating output directory: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c7e0d9862e43fd98f345ea21e0429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "(1, 512, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "csv_path = path + 'train_10perc_v1.csv'\n",
    "source_path = path + 'train_original/'\n",
    "output_path = path + 'train_10perc_v1_green_v2/'\n",
    "channels = ['green'] # can also leave this out since default is 'green'\n",
    "\n",
    "# run\n",
    "dataset_gen(csv_path, source_path, output_path, channels = channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Downloads\\Data\\train_10perc_v1_green\n",
      "working on a dataset with length: 3127\n"
     ]
    }
   ],
   "source": [
    "stats_name = \"Kaggle2018_train_all.dict\"\n",
    "main_stats = Statistics.one_dataset(output_path, save_name = stats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "(array([0.00021]), array([0.00044]))\n"
     ]
    }
   ],
   "source": [
    "print(len(main_stats[0]))\n",
    "print(len(main_stats[1]))\n",
    "print(main_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"ImageJ=1.52b\n",
    "# images=2\n",
    "# slices=2\n",
    "# unit=micron\n",
    "# spacing=0.3\n",
    "# loop=false\n",
    "# min=0.0\n",
    "# max=65535.0\"\"\"\n",
    "\n",
    "# path_yeast = \"G:/Downloads/Data/\"\n",
    "# file = path_yeast + \"num1_WP_E1_S0_F1_I5_C3_A0.tifstack.tif\"\n",
    "# image = tiff.imread(str(file))\n",
    "# print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on a dataset with length: 10\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "i = 0\n",
    "\n",
    "for file in Path(output_path).iterdir():\n",
    "    while i < 10:\n",
    "        if \".tif\" in str(file):\n",
    "            image = tiff.imread(str(file))\n",
    "            #print(image.shape)\n",
    "            images.append(image)\n",
    "        i+=1\n",
    "print(f\"working on a dataset with length: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[[  0   0   0 ...   0   2   1]\n",
      "  [  0   0   0 ...   0   1   0]\n",
      "  [  0   0   0 ...   0   3   0]\n",
      "  ...\n",
      "  [138 101  16 ...   4  15  25]\n",
      "  [188 147  59 ...  17  40  18]\n",
      "  [167 116 218 ...  34  13  14]]]\n",
      "2621440\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(images[0])\n",
    "\n",
    "data = []\n",
    "for im in images:\n",
    "    for j in im:\n",
    "        for k in j:\n",
    "            for l in k:\n",
    "                data.append(l)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004309480427764356\n",
      "0.0005540388663654671\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(images)/65536)\n",
    "print(np.std(images)/65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "def _ss(data):\n",
    "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
    "    c = mean(data)\n",
    "    ss = sum((x-c)**2 for x in data)\n",
    "    return ss\n",
    "\n",
    "def stddev(data, ddof=0):\n",
    "    \"\"\"Calculates the population standard deviation\n",
    "    by default; specify ddof=1 to compute the sample\n",
    "    standard deviation.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError('variance requires at least two data points')\n",
    "    ss = _ss(data)\n",
    "    pvar = ss/(n-ddof)\n",
    "    return pvar**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004309480427764356\n",
      "0.0005540388663842037\n"
     ]
    }
   ],
   "source": [
    "print(mean(data)/65536)\n",
    "print(stddev(data)/65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast = tiff.imread(str('datasets/yeast_v11.1/train/02_WT/WT_WP_E2_Mito0_S2_F1_I1_C1_A0.tifstack.tif'))\n",
    "yeast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
