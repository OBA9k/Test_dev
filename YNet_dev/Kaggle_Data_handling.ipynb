{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from pathlib import Path\n",
    "\n",
    "from resources.conv_learner import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "# path = \"datasets/Kagg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "Below are two functions which: \n",
    "\n",
    "a) select from the original csv a certain percentage of images at random and save them in a second csv<br/>\n",
    "b) based on an input csv, select image files, compose stacks of images according to selected channels and save them in a separate folder as tiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_subset_csv(path, infile, outfile, percentage = 10, seed=None):\n",
    "\n",
    "    perc = math.ceil((int(100 / percentage)))\n",
    "    if seed: random.seed = seed # sharing outfiles directly might be easier\n",
    "    rand_subset = []\n",
    "    print(f\"Generating random {perc}% subset of {infile}...\")\n",
    "    \n",
    "    with open(path + infile, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        train_data = np.vstack(list(reader))\n",
    "\n",
    "    for idx, file in enumerate(train_data[:,0]):\n",
    "        number = random.randint(1,perc)\n",
    "        if number == 1:\n",
    "            rand_subset.append(train_data[idx])\n",
    "    \n",
    "    random_subset = np.vstack(rand_subset)\n",
    "    \n",
    "    # write to csv in the same directory:\n",
    "    # some error-catching might be nice if outfile already exists\n",
    "    df = pd.DataFrame(random_subset)    \n",
    "    df.to_csv(path + outfile, header = False, index = False)\n",
    "    print(f\"Success: saved random {perc}% subset to {outfile}\")\n",
    "    \n",
    "    return random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random 10% subset of train.csv...\n",
      "Success: saved random 10% subset to train_10perc_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# generate random subset csv file:\n",
    "\n",
    "random_ten_percent = generate_random_subset_csv(path, 'train.csv', 'train_10perc_v2.csv', seed = 1)\n",
    "# print(random_ten_percent[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for consistency between subset and source\n",
    "\n",
    "a = list(train_data[:,0])\n",
    "b = list(random_ten_percent[:,0])\n",
    "\n",
    "c = set(a) & set(b)\n",
    "assert len(b) == len(c), \"Selected images not subset of original data\"\n",
    "print('Check successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset \n",
    "\n",
    "dataset_gen():\n",
    "\n",
    "1) Generates composite images (if multiple channels have been selected) from the individual blue, green, yellow and red channels of the original images.<br/>\n",
    "2) The input csv defines the exact images to be selected.<br/>\n",
    "3) Images are stored in a newly generated folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_gen(csv_path, source_path, output_path, channels = ['green']):\n",
    "    \n",
    "    with open(csv_path, 'r') as d:\n",
    "        reader = csv.reader(d)\n",
    "        fnames = np.vstack(list(reader))[1:,0] #### change depending on header ###\n",
    "    \n",
    "    assert not os.path.isdir(output_path), 'Chosen output directory already exists!'\n",
    "    print(f\"creating output directory: {os.path.basename(output_path)}\")\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    for f in tqdm(fnames, total = len(fnames), unit=\"files\"):\n",
    "        im_ = [cv2.imread(str(source_path + f + '-' + c + '.png'), cv2.IMREAD_GRAYSCALE) for c in channels]\n",
    "        im = np.stack(im_)\n",
    "        # im = np.rollaxis(im,0,3)\n",
    "        \n",
    "        assert im.dtype == 'uint8' # making sure files are expected dtype; may change for different datasets...\n",
    "        tiff.imsave(output_path + f + '.tiff', im, imagej = True) # imageJ = True generates ImageJ compatible Hyperstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74606"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating output directory: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52061411990f4a8bb3d20e6aadee7457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=66921), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "path = \"datasets/HPA_challenge_2018/\"\n",
    "\n",
    "csv_path = path + 'HPAv18_60x_def_dupes_removed.csv'\n",
    "source_path = path + 'HPAv18_BGRY_source/'\n",
    "output_path = path + 'HPAv18_GRBY_60x_def_dupes_removed_all/'\n",
    "channels = ['green','red','blue', 'yellow'] # can also leave this out since default is 'green'\n",
    "\n",
    "dataset_gen(csv_path, source_path, output_path, channels = channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ls: Argument list too long\r\n",
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/HPAv18_BGRY_source/ -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPA_labels.csv\t\t\t     models\r\n",
      "HPAv18RBGY_wodpl.csv\t\t     needles.csv\r\n",
      "HPAv18_60x_def_dupes_removed.csv     sample_submission.csv\r\n",
      "HPAv18_BGRY_source\t\t     test-raw\r\n",
      "HPAv18_BGR_all\t\t\t     test.zip\r\n",
      "HPAv18_BGR_source\t\t     test_BGR_all\r\n",
      "HPAv18_def_dupes_removed.csv\t     test_BGYR_all\r\n",
      "HPAv18_dupes_400.csv\t\t     tmp\r\n",
      "HPAv18_dupes_5k.csv\t\t     train.csv\r\n",
      "HPAv18_dupes_no_si_dist5.csv\t     train.zip\r\n",
      "HPAv18_dupes_no_si_dist5.pkl\t     train_BGR_all\r\n",
      "HPAv18_dupes_no_si_phash_10_BGR.pkl  train_BGYR_all\r\n",
      "HPAv18_wodpl_60x.csv\t\t     train_raw\r\n",
      "haystack_no_si.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/HPA_challenge_2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = os.listdir('datasets/HPA_challenge_2018/HPAv18_BGR_all')\n",
    "tiff.imread('datasets/HPA_challenge_2018/HPAv18_BGR_all/' + d[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512, 512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf datasets/HPA_challenge_2018/HPAv18_BGR_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create non-header version of input .csv\n",
    "\n",
    "# csv_path = path + 'HPA_labels.csv'\n",
    "\n",
    "\n",
    "# with open(csv_path, 'r') as d:\n",
    "#     reader = csv.reader(d)\n",
    "#     fnames = np.vstack(list(reader))\n",
    "    \n",
    "# no_header = fnames[1:] # remove header line\n",
    "\n",
    "# no_header_df = pd.DataFrame(no_header)\n",
    "# no_header_df\n",
    "\n",
    "# no_header_df.to_csv(path + 'HPA_labels.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling test-set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_gen(path, test_folder, output_path, channels = ['green']):\n",
    "    \n",
    "    _fnames = test_names = read_dir(path, test_folder)\n",
    "    _fnames = pd.DataFrame([os.path.basename(_fnames[i].split('_')[0]) for i, _ in enumerate(_fnames)])\n",
    "    fnames = _fnames[0].unique();\n",
    "    \n",
    "    assert not os.path.isdir(output_path), 'Chosen output directory already exists!'\n",
    "    print(f\"creating output directory: {os.path.basename(output_path)}\")\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    for f in tqdm(fnames, total = len(fnames), unit=\"files\"):\n",
    "        im_ = [cv2.imread(str(path + test_folder + f + '_' + c + '.png'), cv2.IMREAD_GRAYSCALE) for c in channels]\n",
    "        im = np.stack(im_)\n",
    "        # im = np.rollaxis(im,0,3)\n",
    "\n",
    "        assert im.dtype == 'uint8' # making sure files are expected dtype; may change for different datasets...\n",
    "        tiff.imsave(output_path + f + '.tiff', im, imagej = True) # imageJ = True generates ImageJ compatible Hyperstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating output directory: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6e8ff6c9494720b257655e76e2951a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11702), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### - use generated subset csv to generate folder with tiff files composed of select channels\n",
    "\n",
    "path = path\n",
    "test_folder = 'test-raw/'\n",
    "output_path = path + 'test_BGR_all/'\n",
    "channels = ['blue','green', 'red'] # can also leave this out since default is 'green'\n",
    "\n",
    "testset_gen(path, test_folder, output_path, channels = channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources WIP/Code storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast = tiff.imread(str('datasets/yeast_v11.1/train/02_WT/WT_WP_E2_Mito0_S2_F1_I1_C1_A0.tifstack.tif'))\n",
    "yeast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"ImageJ=1.52b\n",
    "# images=2\n",
    "# slices=2\n",
    "# unit=micron\n",
    "# spacing=0.3\n",
    "# loop=false\n",
    "# min=0.0\n",
    "# max=65535.0\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
