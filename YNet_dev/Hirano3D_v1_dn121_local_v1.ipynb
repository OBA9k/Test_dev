{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from resources.conv_learner import *\n",
    "from resources.plots import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/Hirano3D/\"\n",
    "# data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_clss = {'NM17-002': 0,\n",
    "         'Ctrl-AK': 0,\n",
    "         'GUK1-D': 1, \n",
    "         'GUK1-R': 1,\n",
    "         'MFN2-1': 2,\n",
    "         'VCP-1': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV generation via sklearn, Multilabel implementation by trent-b, or FastAi\n",
    "\n",
    "Both libraries seem to be completely useless... can just use native fastai function: get_cv_idxs().<br>\n",
    "However, get_cv_idxs() does NOT shuffle...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def get_label_stratified_CV_idxs(csv_path):\n",
    "    \n",
    "    _all_labels = pd.read_csv(csv_path)\n",
    "    arr = _all_labels.values\n",
    "\n",
    "    X = arr[:,0]\n",
    "    y = arr[:,1:]\n",
    "    \n",
    "    ### sklearn.model_selection.StratifiedKFold\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "    \n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{pd.Series(arr[:,1][trn_idxs]).value_counts()}\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{pd.Series(arr[:,1][val_idxs]).value_counts()}\"\"\")\n",
    "    \n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "def get_label_stratified_CV_idxs_multi(csv_path):\n",
    "    \n",
    "    # FastAi csv_source expects a folder-name string to be passed as first arg... -> 'dummy'\n",
    "    X, y, all_lbls = csv_source('dummy', csv_path)\n",
    "    \n",
    "    ### Iterative stratification library: https://github.com/trent-b/iterative-stratification\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    \n",
    "    for train_index, val_index in msss.split(X, y):\n",
    "        trn_idxs = train_index\n",
    "        val_idxs = val_index\n",
    "    \n",
    "    trn_count = np.sum(y[trn_idxs], axis=0)\n",
    "    val_count = np.sum(y[val_idxs], axis=0)\n",
    "    \n",
    "    print(f\"\"\"Train label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{trn_count}\"\"\")\n",
    "    print(f\"\"\"Val label-distribution:\\n\"\"\"\n",
    "          f\"\"\"{val_count}\"\"\")\n",
    "    \n",
    "    return trn_idxs, val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label-distribution:\n",
      "0    216\n",
      "1    163\n",
      "2    161\n",
      "3    124\n",
      "dtype: int64\n",
      "Val label-distribution:\n",
      "0    54\n",
      "1    41\n",
      "2    40\n",
      "3    31\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### used atm...\n",
    "csv_path = path + '16bit_multi_folder_Hirano3D_v1.csv'\n",
    "trn_idxs, val_idxs = get_label_stratified_CV_idxs(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# csv_path = path + 'multi_folder_Hirano3D_02.csv'\n",
    "# trn_idxs, val_idxs = get_label_stratified_CV_idxs(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting up a list of a random 20% of images in train as the validation set\n",
    "\n",
    "# lbl_csv = path + 'multi_folder_Hirano3D_02.csv'\n",
    "# n = len(list(open(lbl_csv))) -1\n",
    "# val_idxs = get_cv_idxs(n, val_pct=0.2)\n",
    "\n",
    "\n",
    "# # Count labels\n",
    "# # _all_labels = pd.read_csv(PATH + 'multi_folder_Hirano3D_02.csv')\n",
    "# # print(_all_labels.Targets.value_counts())\n",
    "\n",
    "# _arr = _all_labels.values\n",
    "# _val_labels = pd.DataFrame(_arr[val_idxs], columns=['Id','Targets'])\n",
    "# print(_val_labels.Targets.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, lbl_csv, val_idxs, sz, bs, aug_tfms):\n",
    "\n",
    "    tfms = tfms_with_IntNorm(sz, aug_tfms=aug_tfms, crop_type=CropType.CENTER)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'data', lbl_csv, \n",
    "                                        val_idxs = val_idxs,\n",
    "                                        test_name='data/16bit_Hirano3D_MaxP_GRFB_test_512', \n",
    "                                        tfms=tfms, bs=bs, suffix = '.tif', \n",
    "                                        balance=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"datasets/Hirano3D\"\n",
    "# data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "bs = 64\n",
    "sz = [4,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weights...\n",
      "one-hot encoding single-labels...\n",
      "Weights calculated successfully!\n",
      "Using WeightedRandomSampler\n"
     ]
    }
   ],
   "source": [
    "# define augmentations\n",
    "augs = [RandomDihedral()]\n",
    "\n",
    "# csv:\n",
    "lbl_csv = path + '16bit_multi_folder_Hirano3D_v1.csv'\n",
    "\n",
    "# initialize data object\n",
    "data = get_data(PATH, lbl_csv, val_idxs, sz, bs, aug_tfms = augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inpsect data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.fix_dl))\n",
    "# x_test, y_test = next(iter(data.test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print transformations\n",
    "# plt.style.use('seaborn-white')\n",
    "\n",
    "c = 0\n",
    "idx = 0\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    x, y = next(iter(data.aug_dl))\n",
    "    im = to_np(x)[idx]\n",
    "#     ax.imshow(np.sum(im, axis = 0))\n",
    "    ax.imshow(im[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_raw = tiff.imread('datasets\\\\Hirano3D\\\\test_out/fCMT2_02_e3_017.tif')\n",
    "im_scaled = (im_raw/255)\n",
    "\n",
    "im_sc_rot = np.moveaxis(im_scaled, 0,-1)\n",
    "\n",
    "m = np.array(np.mean(im_sc_rot, axis=(0,1)), dtype=np.float32)\n",
    "s = np.array(np.std(im_sc_rot, axis=(0,1)), dtype=np.float32)\n",
    "\n",
    "im_scaled_normed = (im_sc_rot - m) / s\n",
    "m_n = np.mean(im_scaled_normed[0])\n",
    "s_n = np.std(im_scaled_normed[0])\n",
    "\n",
    "print(np.mean(im_raw[0]))\n",
    "print(np.mean(im_scaled[0]))\n",
    "print(np.max(im_scaled_normed[0]))\n",
    "print(m)\n",
    "\n",
    "print(m_n, s_n)\n",
    "\n",
    "im_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff.imshow(im_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect images\n",
    "plt.style.use('seaborn-white')\n",
    "ch = 0\n",
    "idx = 0\n",
    "\n",
    "# im = to_np(x)[idx]\n",
    "im_rawHPA = tiff.imread('datasets\\\\Kaggle_HPA_2018\\\\HPAv18_BGR_test\\\\1183_51_A11_2.tiff')\n",
    "im_raw = tiff.imread('datasets\\\\Hirano3D\\\\test_out/fCMT2_02_e3_017.tif')\n",
    "# im_scaled = (im_raw/255)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "ax[0].imshow(im_rawHPA[0])\n",
    "ax[1].imshow(im_raw[ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 1\n",
    "\n",
    "print(np.max(im_raw[ch]))\n",
    "print(np.max(im_rawHPA[ch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im_rawHPA[ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_np(x)[5][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading epoch for manual inspection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epoch(dl = data.trn_dl):\n",
    "    \n",
    "    batch = iter(dl)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "#     for b in range(0,len(dl)):\n",
    "    for b in range(0,10):\n",
    "\n",
    "        x_, y_ = next(batch)\n",
    "        x_np, y_np = to_np(x_), to_np(y_)\n",
    "        xs.append(x_np)\n",
    "        ys.append(y_np)\n",
    "\n",
    "    return np.vstack(xs), np.concatenate(ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inpsecting loaded images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(y):\n",
    "    ind = [i for i, p in enumerate(y) if y[i]==1]\n",
    "    return(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect train images\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "idx = 2\n",
    "\n",
    "im = to_np(x)[idx]\n",
    "\n",
    "lbl = to_np(y)[idx]\n",
    "# lbl = to_label(to_np(y)[idx])\n",
    "print(lbl)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    ax.imshow(im[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect test images\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "idx = 3\n",
    "\n",
    "im = to_np(x_test)[idx]\n",
    "\n",
    "# lbl = to_label(to_np(y)[idx])\n",
    "# print(lbl)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,10))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    ax.imshow(im[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_STEP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base-model\n",
    "\n",
    "arch = dn121_c\n",
    "# arch = resnet50\n",
    "learn = ConvLearner.pretrained(arch, data, ps=0, opt_fn=optim.Adam, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional parameters\n",
    "\n",
    "wd=1e-5 # weight-decay/L2 regularization \n",
    "# learn.metrics = [accuracy, f1_micro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7695fd1bcc4c9aa64ee6bc113191fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 ---------------------------------------- STEP 0                                                                                                                                                                                      \n",
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      1.509949   1.437895   0.204819  \n",
      "EPOCH 1 ---------------------------------------- STEP 1                                                                                                                                                                                      \n",
      "    1      1.381894   2.319068   0.204819  \n",
      "\n",
      "creating log-files...\n",
      "log-files saved to: datasets/Hirano3D\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.31907]), 0.20481927746749787]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-3, 2, cycle_len=1, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5b130aa9943d9ba111e16cad15a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 ---------------------------------------- STEP 2                                                                                                                                                                                      \n",
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      1.19584    2.765112   0.313253  \n",
      "EPOCH 1 ---------------------------------------- STEP 3                                                                                                                                                                                      \n",
      "    1      1.134962   2.367119   0.204819  \n",
      "\n",
      "appending existing log-files...\n",
      "log-files saved to: datasets/Hirano3D\n",
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.36712]), 0.20481927746749787]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-3, 2, cycle_len=1, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_per_iter = [[i, learn.sched.losses[idx], learn.sched.lrs[idx]] for idx, i in enumerate(learn.sched.iterations)]\n",
    "_per_iter = pd.DataFrame(per_iter, columns=['Iteration', 'trn_loss', 'lr'])\n",
    "per_iter_df.append(_per_iter)\n",
    "\n",
    "if learn.sched.glob_step: \n",
    "    per_iter_df = _per_iter\n",
    "    print('created')\n",
    "else: \n",
    "    per_iter_df.append(_per_iter)\n",
    "    print('appended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 2, cycle_len=1, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.Glob_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-4, 8, cycle_len=1, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-5, 4, cycle_len=8, wds=wd, use_wd_sched=True, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-4, 2, cycle_len=16, cycle_mult=2, wds=wd, use_wd_sched=True, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(2e-5, 1, cycle_len=8, cycle_mult=2, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (5e-4, 1e-3, 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = [4,516, 516]\n",
    "learn.set_data(get_data(PATH, lbl_csv, val_idxs, sz, bs, aug_tfms = augs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 1, cycle_len=8, cycle_mult=2, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 2, cycle_len=8, wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('Hirano3D_v1_ResNet18_512_72')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('Hirano3D_v1_ResNet18_224_70')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.predict_with_targs()\n",
    "preds = np.argmax(log_preds, axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_preds, y =  learn.TTA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "plt.style.use('seaborn-white')\n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "cm = confusion_matrix(preds,y)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y =  learn.TTA(is_test=True)\n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be called by register_forward_hook\n",
    "\n",
    "def get_embeddings(layer_name):\n",
    "    def register_hook(layer_name):\n",
    "        def get_embedding(layer, inp, outp):\n",
    "            tmp = inp[0]\n",
    "            embedding.append(tmp)\n",
    "\n",
    "        hook = layer.register_forward_hook(get_embedding) \n",
    "        \n",
    "        for i in ['trn', 'val', 'test']:\n",
    "            embedding = []\n",
    "            preds, y = learn.predict_with_targs(i)\n",
    "            \n",
    "            # populating dict, consiting of [0]: preds, [1]: y, [2]: activations[layer]\n",
    "            embeddings[i] = [preds, y, np.vstack(to_np(embedding))]\n",
    "            \n",
    "        hook.remove()\n",
    "        \n",
    "    embeddings = {}    \n",
    "    layer = learn.models.model._modules.get(layer_name)\n",
    "    register_hook(layer)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings('14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking embeddings\n",
    "\n",
    "embs_trn = embeddings['trn'][2]\n",
    "y_trn = embeddings['trn'][1]\n",
    "\n",
    "embs_val = embeddings['val'][2]\n",
    "y_val = embeddings['val'][1]\n",
    "\n",
    "embs_test = embeddings['test'][2]\n",
    "y_test = embeddings['test'][1]\n",
    "\n",
    "print(embs_trn.shape)\n",
    "print(embs_val.shape)\n",
    "print(embs_test.shape)\n",
    "\n",
    "print(y_trn.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_trn_embedding = umap.UMAP(n_neighbors=10,\n",
    "                      min_dist=0.3,\n",
    "                      metric='correlation').fit(embs_trn)\n",
    "\n",
    "UMAP_trn = UMAP_trn_embedding.embedding_\n",
    "UMAP_val = UMAP_trn_embedding.transform(embs_val)\n",
    "UMAP_test = UMAP_trn_embedding.transform(embs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(60)\n",
    "y_test[:30] = y_test[:30] +1 \n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting PCA vs TSNE results\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "compA = 0\n",
    "compB = 1\n",
    "\n",
    "for i in range(4):\n",
    "    trn_UMAP_cls = UMAP_trn[y_trn == i]\n",
    "\n",
    "    axarr[0].scatter(trn_UMAP_cls[:,compA], trn_UMAP_cls[:,compB], label = data.classes[i], s = 5)\n",
    "    axarr[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    if i == 0:\n",
    "        axarr[1].scatter(trn_UMAP_cls[:,compA], trn_UMAP_cls[:,compB], label = data.classes[i], s = 5)\n",
    "\n",
    "    axarr[0].set_xlim(-10,5)\n",
    "    axarr[0].set_ylim(-5,8)\n",
    "    \n",
    "for i in [0,1]:\n",
    "    test_UMAP_cls = UMAP_test[y_test == i]\n",
    "    \n",
    "    axarr[1].scatter(test_UMAP_cls[:,compA], test_UMAP_cls[:,compB], s = 5)\n",
    "    axarr[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axarr[1].set_xlim(-10,5)\n",
    "    axarr[1].set_ylim(-5,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_th(preds, targs, start=0.2, end=0.6, step=0.05):\n",
    "    ths = np.arange(start,end,step)\n",
    "    res = [f1_macro(preds, targs, thresh=th, kind='macro') for th in ths]\n",
    "    idx = np.argmax(res)\n",
    "    return ths[idx], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train predictions\n",
    "\n",
    "preds_trn, targs_trn =  learn.predict_with_targs('trn')\n",
    "\n",
    "preds_trn_torch = torch.from_numpy(preds_trn)\n",
    "targs_trn_torch = torch.from_numpy(targs_trn)\n",
    "\n",
    "opt_th(preds_trn_torch, targs_trn_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_val_torch[0])\n",
    "print(preds_trn_torch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get val predictions\n",
    "\n",
    "preds_val, targs_val =  learn.predict_with_targs('val')\n",
    "\n",
    "preds_val_torch = torch.from_numpy(preds_val)\n",
    "targs_val_torch = torch.from_numpy(targs_val)\n",
    "\n",
    "opt_th(preds_val_torch, targs_val_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions\n",
    "\n",
    "preds_test, targs_test =  learn.predict_with_targs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs_test[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epoch(dl = data.trn_dl):\n",
    "    \n",
    "    batch = iter(dl)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "#     for b in range(0,len(dl)):\n",
    "    for b in range(0,10):\n",
    "\n",
    "        x_, y_ = next(batch)\n",
    "        x_np, y_np = to_np(x_), to_np(y_)\n",
    "        xs.append(x_np)\n",
    "        ys.append(y_np)\n",
    "\n",
    "    return np.vstack(xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_dl.sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = data.test_ds.fnames\n",
    "test_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(preds_file, output_name, th = 0.3, TTA=False):\n",
    "    \n",
    "    # creating submission file\n",
    "    \n",
    "    if TTA:\n",
    "        preds = preds_file.mean(axis=0)\n",
    "        print('TTA:',preds.shape)\n",
    "    else: preds = preds_file\n",
    "    \n",
    "    clss = np.arange(0, len(data.classes)) # get class indeces\n",
    "    res = np.array([' '.join(np.char.mod('%d', clss[np.where(p > th)])) for p in preds]) # generating output\n",
    "\n",
    "    # ensure that there are no empty cells: in case no value > thresh, fill in with argmax()\n",
    "    for i in range(res.shape[0]):\n",
    "        if res[i] == '':\n",
    "            res[i] = preds[i].argmax()\n",
    "\n",
    "    # getting image Ids\n",
    "    fnames = np.array([os.path.basename(im).split('.')[0] for im in data.test_ds.fnames])\n",
    "\n",
    "    # creating submission file\n",
    "    sub_df = pd.DataFrame(res, index=fnames, columns=['Predicted'])\n",
    "    sub_df.to_csv(output_name, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = PATH + 'submissions/Res18_pre_0.42_t-03.csv'\n",
    "\n",
    "create_submission(log_preds, submission_name, th=0.3, TTA=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label identities:\n",
    "\n",
    "cell_location_label = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# thresholds = np.linspace(0, 1, 1000)\n",
    "# score = 0.0\n",
    "# test_threshold=0.5*np.ones(28)\n",
    "# best_threshold=np.zeros(28)\n",
    "# best_val = np.zeros(28)\n",
    "# for i in range(28):\n",
    "#     for threshold in thresholds:\n",
    "#         test_threshold[i] = threshold\n",
    "#         max_val = np.max(preds_y)\n",
    "#         val_predict = (preds_y > test_threshold)\n",
    "#         score = f1_score(valid_y > 0.5, val_predict, average='macro')\n",
    "#         if score > best_val[i]:\n",
    "#             best_threshold[i] = threshold\n",
    "#             best_val[i] = score\n",
    "#     print(\"Threshold[%d] %0.6f, F1: %0.6f\" % (i,best_threshold[i],best_val[i]))\n",
    "#     test_threshold[i] = best_threshold[i]\n",
    "# print(\"Best threshold: \")\n",
    "# print(best_threshold)\n",
    "# print(\"Best f1:\")\n",
    "# print(best_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
